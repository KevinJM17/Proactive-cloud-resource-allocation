# -*- coding: utf-8 -*-
"""data_preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i6cWfgnpCUHmqPix641K5lyrgHNGrsc2
"""

# Import packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import glob
from pandas import read_csv, datetime
from pandas.plotting import autocorrelation_plot
from dateutil.relativedelta import relativedelta
from scipy.optimize import minimize
import statsmodels.formula.api as smf
import statsmodels.tsa.api as smt
import statsmodels.api as sm
import scipy.stats as scs
from sklearn.linear_model import LassoCV, RidgeCV
from itertools import product
from tqdm import tqdm_notebook
import matplotlib.dates as mdates
# %matplotlib inline

import warnings
warnings.filterwarnings('ignore')

# Import required packages
import os
import glob
import pandas as pd

def load_and_concatenate(path):
    # Get a list of all .csv files in the specified directory
    all_files = glob.glob(os.path.join(path, "*.csv"))

    # For each file, read it into a DataFrame and add a new column 'VM'
    # that contains the filename (without the .csv extension)
    df_from_each_file = (pd.read_csv(f, sep=';\t').assign(VM=os.path.basename(f).split('.')[0]) for f in all_files)

    # Concatenate all the DataFrames together
    concatenated_df = pd.concat(df_from_each_file)

    return concatenated_df

# Load and concatenate data from the directories for July, August, and September
df_july = load_and_concatenate('rnd/2013-7/')
df_august = load_and_concatenate('rnd/2013-8/')
df_september = load_and_concatenate('rnd/2013-9/')

# Combine all three months of data
concatenated_df = pd.concat([df_july, df_august, df_september])

# Display the first few rows of the DataFrame
concatenated_df.head()

concatenated_df['Timestamp'] = pd.to_datetime(concatenated_df['Timestamp [ms]'], unit = 's')
concatenated_df.apply(pd.to_numeric, errors='ignore')

# Date Feature Engineering
concatenated_df['weekday'] = concatenated_df['Timestamp'].dt.dayofweek
concatenated_df['weekend'] = ((concatenated_df.weekday) // 5 == 1).astype(float)
concatenated_df['month']=concatenated_df.Timestamp.dt.month
concatenated_df['day']=concatenated_df.Timestamp.dt.day
concatenated_df.set_index('Timestamp',inplace=True)

# Other Feature Engineering
concatenated_df["CPU usage prev"] = concatenated_df['CPU usage [%]'].shift(1)
concatenated_df["CPU_diff"] = concatenated_df['CPU usage [%]'] - concatenated_df["CPU usage prev"]

# Performing forward fill on missing values
concatenated_df = concatenated_df.fillna(method='ffill')

concatenated_df.head()
hourlydat = concatenated_df.resample('H').sum()
hourlydat.to_csv('hourlydata.csv')
hourlydat.head()

concatenated_df['start'] = concatenated_df.index
concatenated_df['target'] = concatenated_df['CPU usage [MHZ]']

df2 = concatenated_df.groupby('VM').resample('1min')['target'].mean().to_frame()
# df2.reset_index(level=0, inplace=True)

df2.head()

df2 = df2.fillna(method='ffill')

df2.to_csv('df2.csv')

df3 = concatenated_df.groupby('VM').resample('1min')['CPU capacity provisioned [MHZ]'].mean().to_frame()
# df3.reset_index(level=0, inplace=True)
df3 = df3.fillna(method='ffill')
df3.head()

df3.to_csv('df3.csv')

'''CPU Capacity Provisioning and Usage Analysis'''
overprovision = pd.DataFrame(hourlydat['CPU usage [MHZ]'])
overprovision['CPU capacity provisioned'] = pd.DataFrame(hourlydat['CPU capacity provisioned [MHZ]'])

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd


sns.set_style("whitegrid")
fig, ax = plt.subplots(figsize=(12, 6))
sns.lineplot(x=overprovision.index, y='CPU usage [MHZ]', data=overprovision, ax=ax, label='CPU usage [MHZ]', color='steelblue', linewidth=2.5)
sns.lineplot(x=overprovision.index, y='CPU capacity provisioned', data=overprovision, ax=ax, label='CPU capacity provisioned [MHZ]', color='tomato', linewidth=2.5)
ax.set_title('CPU Capacity and Usage Comparison')
ax.set_ylabel((r'CPU [MHz]  $e^{7}$'))
ax.set_xlabel('Date')
ax.legend(loc='best')
ax.ticklabel_format(axis='y', style='sci', scilimits=(1,6))
plt.savefig('dataset_cpu_usage.png')
plt.show()

'''Perform KPSS stationarity test'''
def kpss_test(timeseries):
    statistic, p_value, n_lags, critical_values = kpss(timeseries, regression='ct')
    # Format Output
    print(f'KPSS Statistic: {statistic}')
    print(f'p-value: {p_value}')
    print(f'num lags: {n_lags}')
    print('Critial Values:')
    for key, value in critical_values.items():
        print(f'   {key} : {value}')
    print(f'Result: The series is {"not " if p_value < 0.05 else ""}stationary')

kpss_test(overprovision['CPU usage [MHZ]'])

'''Data pre-processing for DeepAR and Chronos'''
# Data processing: Loading and concatenating datasets
files = glob.glob(os.path.join('rnd/2013-7', "*.csv"))

files_first200 = files[:150]
dfs = [pd.read_csv(fp, sep = ';\t').assign(VM=os.path.basename(fp).split('.')[0]) for fp in files_first200]
df = pd.concat(dfs, ignore_index=True)

files2 = glob.glob(os.path.join('rnd/2013-8', "*.csv"))
files2_first200 = files2[:150]
dfs2 = [pd.read_csv(fp, sep = ';\t').assign(VM=os.path.basename(fp).split('.')[0]) for fp in files2_first200]
df2 = pd.concat(dfs2, ignore_index=True)

files3 = glob.glob(os.path.join('rnd/2013-9', "*.csv"))
files3_first200 = files3[:150]
dfs3 = [pd.read_csv(fp, sep = ';\t').assign(VM=os.path.basename(fp).split('.')[0]) for fp in files3_first200]
df3 = pd.concat(dfs3, ignore_index=True)

newdat = pd.concat([df, df2])
concatenated_df = pd.concat([newdat, df3])

concatenated_df.head()

# Formatting
concatenated_df['Timestamp'] = pd.to_datetime(concatenated_df['Timestamp [ms]'], unit='s')
concatenated_df.describe()
concatenated_df['weekday'] = concatenated_df['Timestamp'].dt.dayofweek
concatenated_df['weekend'] = ((concatenated_df.weekday) // 5 == 1).astype(float)

# Feature engineering with the date
concatenated_df['month'] = concatenated_df.Timestamp.dt.month
concatenated_df['day'] = concatenated_df.Timestamp.dt.day
concatenated_df.set_index('Timestamp', inplace=True)
concatenated_df["CPU usage prev"] = concatenated_df['CPU usage [%]'].shift(1)
concatenated_df["CPU_diff"] = concatenated_df['CPU usage [%]'] - concatenated_df["CPU usage prev"]
concatenated_df["received_prev"] = concatenated_df['Network received throughput [KB/s]'].shift(1)
concatenated_df["received_diff"] = concatenated_df['Network received throughput [KB/s]']- concatenated_df["received_prev"]
concatenated_df["transmitted_prev"] = concatenated_df['Network transmitted throughput [KB/s]'].shift(1)
concatenated_df["transmitted_diff"] = concatenated_df['Network transmitted throughput [KB/s]']- concatenated_df["transmitted_prev"]

concatenated_df["start"] = concatenated_df.index
concatenated_df['target'] = concatenated_df['CPU usage [MHZ]']

df2 = concatenated_df.groupby('VM').resample('1min')['target'].mean().to_frame()
df2.reset_index(level=0, inplace=True)

df2.head()

df2 = df2.fillna(method='ffill')

df2.to_csv('df2.csv')

df3 = concatenated_df.groupby('VM').resample('1min')['CPU capacity provisioned [MHZ]'].mean().to_frame()
df3.reset_index(level=0, inplace=True)
df3 = df3.fillna(method='ffill')
df3.head()

df3.to_csv('df3.csv')