# -*- coding: utf-8 -*-
"""chronos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w5b7bh36yCOSmYM36_p8vmUmZam7Cl-w
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install --quiet --upgrade s3fs

# Initial setup: Importing necessary libraries and packages
import warnings
warnings.filterwarnings('ignore')

# Set up the S3 bucket and prefix for SageMaker
bucket = 'workload-prediction-v2'
prefix = 'sagemaker/test'

# Set up IAM role and SageMaker session
import sagemaker
import sagemaker.predictor
import boto3
import s3fs
import re
from sagemaker import get_execution_role
import json
import math
from os import path
import sagemaker.amazon.common as smac

role = get_execution_role()
sagemaker_session = sagemaker.Session()
region = boto3.Session().region_name
smclient = boto3.Session().client('sagemaker')

# Define data paths on S3
s3_data_path = "{}/{}/data".format(bucket, prefix)
s3_output_path = "{}/{}/output".format(bucket, prefix)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import io
import os
import time
import json
import glob

d2 = pd.read_csv('df2.csv', index_col='Timestamp')

df = d2.iloc[:-30]
df2_last_30 = d2.iloc[-30:]

from pathlib import Path
from typing import List, Union

import numpy as np
from gluonts.dataset.arrow import ArrowWriter


def convert_to_arrow(
    path: Union[str, Path],
    time_series: Union[List[np.ndarray], np.ndarray],
    compression: str = "lz4",
):
    
    # Store a given set of series into Arrow format at the specified path
    # Input data can be either a list of 1D numpy arrays, or a single 2D
    # numpy array of shape (num_series, time_length).
    assert isinstance(time_series, list) or (
        isinstance(time_series, np.ndarray) and
        time_series.ndim == 2
    )

    ArrowWriter(compression=compression).write_to_file(
        df,
        path=path,
    )


if __name__ == "__main__":
    # Generate 20 random time series of length 1024
    time_series = [np.random.randn(1024) for i in range(20)]

    # Convert to GluonTS arrow format
    convert_to_arrow("./noise-data.arrow", time_series=time_series)

# %pip install -q git+https://github.com/amazon-science/chronos-forecasting.git

import pandas as pd  # requires: pip install pandas
import torch
from chronos import ChronosPipeline

pipeline = ChronosPipeline.from_pretrained(
    "amazon/chronos-t5-small",
    device_map="cpu",  # use "cpu" for CPU inference and "mps" for Apple Silicon
    torch_dtype=torch.bfloat16,
)

# context must be either a 1D tensor, a list of 1D tensors,
# or a left-padded 2D tensor with batch as the first dimension
# forecast shape: [num_series, num_samples, prediction_length]
forecast = pipeline.predict(
    context=torch.tensor(df["target"]),
    prediction_length=30,
    num_samples=30,
)

print(df2_last_30)

forecast.mean(dim=(0, 1))

low, median, high = np.quantile(forecast[0].numpy(), [0.1, 0.5, 0.9], axis=0)
forecast_index = range(len(low))

plt.figure(figsize=(12, 6))
plt.plot(median, label="Median forecast")
plt.fill_between(forecast_index, low, high, alpha=0.3, label="80% prediciton interval")
plt.plot(df2_last_30['target'], label="Actual usage")
plt.title("DeepAR Model Prediction")
plt.ylabel("CPU usage [MHz]")
plt.xlabel("Time")
plt.xticks(rotation=90)
plt.legend()
plt.grid()
plt.show()

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

y_true = df2_last_30['target']
y_pred = median
mse = mean_squared_error(y_true, y_pred)
mae = mean_absolute_error(y_true, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_true, y_pred)
ape = np.abs((np.array(y_true) - np.array(y_pred)) / np.array(y_true))
mape = np.mean(ape) * 100


print('Mean Squared Error:', mse)
print('Mean Absolute Error:', mae)
print('Root Mean Squared Error:', rmse)
print('R2 Score:', r2)
print('Mean Absolute Percentage Error (MAPE):', mape)